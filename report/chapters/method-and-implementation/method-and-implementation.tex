\section{Method/Implementation}
\subsection{Iteration 1: Baseline Setup}
\subsubsection{About}
The initial iteration of this project involved creating a baseline setup based on Antonios Liapis' MiniDungeons project \cite{liapis_minidungeons} ported to Python by ganyariya \cite{GymMd}.
This iteration was used to provide a testbed for evalutating the quality of levels generated by the future level generator. MiniDungeons was used as a testbed as a means of
limiting the scope of the project to generating levels, rather than creating a full game \textit{and} levels.

The Python port of MiniDungeons (\texttt{gym-md}) \cite{GymMd} proved to be a sufficient starting point for this projects goals, however it was created using a legacy version of Gymnasium \cite{Gymnasium}\cite{towers2025gymnasiumstandardinterfacereinforcement}.
For that reason, in order to avoid issues with dependencies, a fork of \texttt{gym-md}\cite{GymMdFork} was created with the express purpose of updating its core dependencies to allow further development.

The final step of this iteration was to setup the project repository to be able to run and modify MiniDungeons from a central location. The project was created using Poetry\cite{PythonPoetry} to manage dependencies and standardize entry points for the code.
The project was setup to extend \texttt{gym-md} while still allowing for new levels to be used in the environment. The iteration concluded with a standard entry point for running
MiniDungeons with hardcoded custom dungeon layouts.

\subsubsection{Evaluation}
As this iteration focused on the technical infrastructure and software architecture rather than a playable artifact, the evaluation consisted of technical verification and functional integration testing rather than user studies.

\textbf{Verification Objectives:} The primary objective was to verify that the updated Gymnasium environment could successfully read and render custom level data, to serve as a reliable testbed for the PCG algorithms to be developed in future iterations.
Specifically, the system needed to pass three criteria:
\begin{enumerate}
	\item \textbf{Dependency Stability:} The project must install deterministically via Poetry without conflict between the legacy gym-md logic and modern Python 3.13 standards.
	\item \textbf{Execution:} The environment must run a simulation loop with agent movement and interactions, which is necessary for testing batch-generated levels.
	\item \textbf{Map Injection:} The system must accept a custom 2D character array (representing a dungeon level) at runtime and correctly initialize the game state based on that array.
\end{enumerate}

\textbf{Testing Methodology:} \texttt{gym-md} came pre-configured with a suite of tests that was used for verifying the updated \texttt{gym-md} fork.
A "Random Agent" baseline was implemented to send random action inputs to the environment for 1000 steps to ensure no runtime errors occurred during state transitions.
To test custom map injections, a hardcoded level containing all possible game elements (walls, monsters, potions, exit) was passed to the environment constructor.

\textbf{Results:} The fork of \texttt{gym-md} successfully executed the simulation loop using the updated Gymnasium API.
The Poetry configuration resolved the dependency graph, eliminating the version conflicts that would have otherwise been present in the development repository.
Most importantly, the map injection test confirmed that the environment could dynamically load layouts provided by an external source. However, in order to run these injected levels it was necessary 
to also create various standalone configuration files alongside each level and therefore this test was only considered partially successful.

\subsubsection{Reflections and decisions}
The results of the technical evaluation confirmed that the baseline setup was stable and ready for procedural content generation integration, however not without some complications regarding iteration speed.

\textbf{Interpretation:} The successful map injection was the most critical outcome of this iteration.
It confirmed that the MiniDungeons environment could be decoupled from the pre-defined level data from within the \texttt{gym-md} package. Running injected levels also required creating various configuration files to manage internal settings of the environment. 
Due to the subject of this project being about procedural content generation, modifying internal settings for each level was deemed outside the scope of the research questions and therefore meant unnecessary overhead for the project.
While this iteration validated the decision to use this specific framework, it was clear that some further modification was needed.

\textbf{Impact on Future Iterations:} Although the baseline structure showed success, the cumbersome nature of creating the required artifacts for the MiniDungeon environment 
lead the next iteration to focus on simplifying the environments loading requirements. In iteration 2, the goal would be to reduce the required artifacts to just the 2D character array representing the level, rather than also creating extra artifacts.
Additonally, the implemented random agent does not provide a sufficient baseline for validating the quality of a generated level, as the data would be fully inconsistent and separeted from the actual level content. Iteration 2 also needed to address this issue before a generator could be created. 

% -------------------------------------------------------------------

\subsection{Iteration 2: Refactored Environment and Agent}
\subsubsection{About}
Second iteration involved refactoring the MiniDungeons environment.

Purpose: Improve our ability to interact with the environment.

Work:
\begin{itemize}
	\item Recreated \texttt{gym-md} environment and moved rendering to pygame.
	\item Create a deterministic agent based on MiniDungeon personas \cite{holmgard2014evolvingpersonas}.
\end{itemize}

\subsubsection{Evaluation}
evaluate

\subsubsection{Reflections and decisions}
reflect and decide

% -------------------------------------------------------------------

\subsection{Iteration 3: Dungeon Generator}
\subsubsection{About}
Third iteration involved procedurally generating dungeon layouts.

\subsubsection{Evaluation}
evaluate

\subsubsection{Reflections and decisions}
reflect and decide

% \subsection{Iteration X}

% You will often go through various iterations of product/game design, duplicate this subsection for each iteration before the final one. 

% \begin{itemize}
% 	\item Usually, iteration 1 would be a low fidelity prototype.
% 	\item Iteration 2 could be a high-fidelity prototype (vertical slice or functional prototype).
% 	\item Iteration 3 or later are refinements (extended functionality, improvements/polish, usability tweaks, bug-fixing) or the final product.
% \end{itemize}

% \subsubsection{About}

% Describe what this iteration was all about.

% \begin{itemize}
% 	\item What is the purpose of this iteration?
% 	\item What were the goals?
% 	\item Describe and justify design decisions. For example, “we implemented feature X because it relates to gamification concept Y”, or “in the previous iteration we identified issues with the user interface, so we reworked by changing X, Y, and Z.”
% 	\item Include images/sketches. 
% \end{itemize}

% \subsubsection{Evaluation}

% Describe how you evaluated this iteration, for example what kind of user testing or technical testing you conducted. This should include:

% \begin{itemize}
% 	\item Test design: explain what is the objective of the study: what kind of information are you trying to obtain? What kind of study did you conduct? (Usability test, playtest, something more related to your RQs, etc.?) Did you use a specific established methodology? Usually this is not a very large part and serves as an introduction to your test.
% 	\item Test methodology: this is usually a larger part where you need to provide all the details of how you conducted your test. 
% 	\begin{itemize}
% 		\item How did you structure the test? For example, “The participant played the game while doing think-aloud, then compiled a survey.”
% 		\item Ethical reflections: is there some problematic aspect that you had to address? If so, how did you address it? How are you compliant with GDPR laws? 
% 		\item Technical details: where did you conduct the study? What hardware was used? 
% 		\item Detailed information about the test. For example, if you used a survey, what questions were included? If you conducted interviews, how did you structure them? How did you collect the data?
% 	\end{itemize}
% 	\item Results of the test
% 	\begin{itemize}
% 		\item How many participants did you have?
% 		\item What are some basic demographics for the participants?
% 		\item Present the data collected (use tables and graphs, rather than text). 
% 		\item Make sure you mention what statistical methods you use (and why).
% 	\end{itemize}
% \end{itemize}

% \subsubsection{Reflections and decisions}
% In this section describe how are you using the results you described in the previous section to further your development, to answer your research questions, or in general to evaluate your project (this last part is mostly relevant for the final iteration)

% \begin{itemize}
% 	\item How do you interpret the results you just described? What do they mean?
% 	\item How are you planning to use these in the following iteration?
% \end{itemize}

% \subsection{Final iteration}

% This section describes the final tweaks you applied from the previous iteration. Show the main differences and describe the features of the final product (just like the “About” section in the previous iterations)
